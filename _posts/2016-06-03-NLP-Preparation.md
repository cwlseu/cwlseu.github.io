---
layout: post
title: NLP Introduction
categories: [blog ]
tags: [NLP, ]
description: 自然语言处理需要大量语料进行学习，而语料的集合往往被称为语料库。

---

# 语料库
@(自然语言处理)[自然语言处理|信息检索]
## 基本概念
###语言数据库或知识库：
1. **大规模语言数据**
* 模型参数训练
* 评测标准
2. **NLP中知识库**
* 词汇语义库
* 词法、句法规则库
* 常识库等等**

###国内语料库现状
| 单位   |    名称   | 规模   |
|:------|:-------|:------|
|武汉大学|现代文学作品语料库|1979， 527万|
|北航|现代汉语语料库|1983， 2000万|
|北师大|中学语文教材语料库|1983,106万|
|北京语言学院|现代汉语词频统计语料库|1983,182万|
|国家语言文字工作委员会|国家级大型汉语语料库|7000万|
|清华大学|汉语歧义切分语料库|1998年,1亿|
此外，[北京大学计算语言学研究所](http://icl.pku.edu.cn/)在俞士汶教授领导下建立的综合型语言知识库(简称CLKB)涵盖了词、词组、句子、篇章各单位和词法、句法、语义各层面，从汉语向多语言辐射，从通用领域深入到专业领域。 CLKB是目前国际上规模最大且获得广泛认可的汉语语言知识资源。


## Question
### 平衡语料库
* 各分布点所选取语料量的科学依据是什么？
* 使用度是否真实地反映了语言的使用情况？

## 语料库构建中需要考虑的问题

* 静态和动态
* 代表性和平衡性
* 规模
* 语料库的管理和维护

## 语料库如何评测可用性呢？
[语言专家评测？那么专家如何平衡？]( http://www.docin.com/p-402755901.html)

#语言模型
@(自然语言处理)[自然语言处理|信息检索]

##基本概念
语料库和**统计方法**的成功使用。
基于大规模语料库和统计方法，可以
－ 发现语言使用的普遍规律
－ 进行机器学习、自动获取语言知识
－ 对未知语言现象进行推测
**统计基元** 
**历史**

使用所有的历史信息，简化为使用n元文法
##n-gram模型
### 定义
1-gram第i位的基元$w_i$独立于历史
2-gram被称为1阶马尔科夫链
3-gram被称为2阶马尔科夫链
例如：
$$p(s) = \prod_{i=1}^{m+1}p(w_i|w^{i-1}_{i-n+1})$$
$$\sum_{i=1}^n p(s) =1$$

### 如何获得n-gram模型
#### 参数估计
**训练语料**：
**最大似然估计**
## 数据匮乏导致零概率问题的处理方式-数据平滑(data smoothing)

调整最大似然估计的概率值,使零概率增值，使非零概率下调， “劫富济贫” ，消除零概率，改进模型的整体正确率。
### Objection
测试样本的语言模型困惑度越小越好。
好，下面对困惑度进行介绍：
对于一个平滑的n-gram，概率$p(w_i|w^{i-1}_{i-n+1})$,那么每个句子出现的概率为$p(s) = \prod_{i=1}^{m+1}p(w_i|w^{i-1}_{i-n+1})$，现在假定我们的训练语料T由$l_T$个句子构成，$(t_1,....,t_{l_T})$,那么整个测试集的概率为： 
$$p(T) = \prod_{i=1}^{l_T}p(t_i)$$
模型$p(w_i|w^{i-1}_{i-n+1})$ 对于测试语料的交叉熵为：
$$H_p(T) = -\frac{1}{W_T}log_2p(T)$$其中$W_T$是测试文本T的词数，那么模型p的困惑度$PP_p(T) = 2^{H_p(T)}$

###数据平滑方法
#### 加1法
$$p(w_i|w_{i-1}) = \frac{1 + c(w_{i-1}w_i)}{\sum_{w_i}[1 + c(w_{i-1}w_i)]} = \frac{1 + c(w_{i-1}w_i)}{|V| + sum_{w_i}c(w_{i-1}w_i)} $$
#### 减值法/折扣法
1. Good-Turing估计
压缩比例，使得$\frac{n_1}{N}$平均分配给0概率事件。然后再进行归一化就可以了。
$$p_r = \frac{p_r}{\sum_rp_r}$$
[证明和推导参考](A. Nadas. on Turing’s Formula for Word Probabilities. In IEEE Trans. on ASSP-33, Dec. 1985. Pages 1414-1416)
2. Back-off(后备/后退)方法
 对于每个计数 r > 0 的n元文法的出现次数减值，把因减值而节省下来的剩余概率根据低阶的 (n-1)gram 分配给未见事件。

3. 绝对减值法
从每个计数 r 中减去同样的量，剩余的概率量由未见事件均分。
[论文参考](H. Ney and U. Essen. Estimating Small Probabilities by Leaving-one-Out. In Proc. Eurospeech’1993. Pages 2239-2242)
4. 线性减值法
从每个计数 r 中减去与该计数成正比的量(减值函数为线性的)，剩余概率量 $\alpha$ 被$n_0$个未见事件均分。

###删除插值法
用低阶语法估计高阶语法，即当 3-gram的值不能从训练数据中准确估计时，用 2-gram 来替代，同样，当 2-gram 的值不能从训练语料中准确估计时，可以用 1-gram 的值来代替。

### 数据平滑方法详细介绍与比较
http://www-2.cs.cmu.edu/~sfc/html/publications.html

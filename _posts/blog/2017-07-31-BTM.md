---
layout: post
title: "BiTerm Topic Model"
categories: [blog ]
tags: [机器学习]
description: 
---
{:toc}

- 声明：本博客欢迎转发，但请保留原作者信息!
- 作者: [曹文龙]
- 博客： <https://cwlseu.github.io/>     

## 文章来源

## 主要用途分析

## 模型

## 实验结果

## 小结

LDA的问题是稀疏性。LDA中每个文档对应一个theta，每个词对应一个z。对于短文本，由于词少，z-->theta这一步的统计可能不具备统计意义。因为每个文本单独对应于theta，所以增加文本数量不能克服这种短文本带来的缺陷。

传统的解决之道有两个。
* 是将多个短文本聚合成一个长文本。比如LDA-U将一个user的多个发言聚合在一起减少稀疏影响。但这种做法与应用相关，需要有外部信息来辅助找到聚合的单位。
* 利用一个长文档文本集作为辅助数据，将长文本的一些信息先以某种形式抽取反应到先验中，从而间接地提供给短文本。看似很美好，问题在于，与transfer learning类似，这个知识能不能传还两说，得先找到一个总体主题分布类似的长文档集。

总体来说,LDA并不适用于短文本，LDA-U略好，但是基于user的聚合受限于应用和领域知识。

mixture of unigram在这里表现比LDA系列更好，原因是它对于所有文档用同一个theta，克服了短文本稀疏性问题。这个模型的问题是，它假设了整个文档对应于一个z，这比较粗糙和strong了，会导致一个topic类内距离很大，因为对于长文本，较远的段落之间可能语义差异很大了。在长文本以及K较大的时候，这个模型效果会比较差，但在短文本上，这个缺陷倒是不那么明显，因此效果好于LDA系列

最后是BTM模型。可以认为BTM综合了上述二者的优点。BTM跟mix一样，利用了整个文本集合来估计一个theta，解决了稀疏问题（我们通常有海量数据）。放宽了mix中对整个doc必须同属于一个z的约束（相当于从整doc放松到了窗口长度类的两个词），加强了LDA中每个词对应于一个Z的假设（BTM中约束了窗长内的两个词组成一个biterm对应于一个z）。这个假设很接近于人类认知，因为我们知道，通常在较短的一段文本内，topic变化不大。

说一句题外话，前几天听了一个知识图谱的讲座。他们也是用一个共现词对来排歧作为LDA的补充。比如 李娜+青藏高原 这个词对，可以很大程度上将李娜和其他人（比如另一个做音乐的李娜，无法被LDA所区分）区分开来。unigram是一个极端，一大堆词是另一个极端了。BTM相当于是这种思路在TM系列模型中的一个探索。从另一个角度，2gram或许也可以替换term作为LDA的单元，而BTM的尝试是将连续的2gram放开，允许中间有一个窗长。总的来说，这是一个相当有意思的思路，而且非常nature。
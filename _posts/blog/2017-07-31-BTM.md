---
layout: post
title: "BiTerm Topic Model"
categories: [blog ]
tags: [机器学习]
description: 
---
* content
{:toc}     

## 需求背景
短文本，短视频的tag标签，如果使用图文的方法，显得力不从心，bad case太多。例如直接使用LDA算法对图文语料进行训练，取得不错的聚类效果，但是使用短视频的标签信息语料进行相同的操作，效果不佳。
直接使用word2vec进行关键词匹配的方式，很大程度上依赖视频标签的质量。而视频标签的标注信息不能依赖于信息上传者的标注信息与用户标注意图，这个不定因素不是我们能够控制的。

## 主题模型发展回顾
### LDA
![@LDA模型](https://cwlseu.github.io/images/btm/LDA-ARCH.png)
LDA的问题是稀疏性。LDA中每个文档对应一个$\theta$，每个词对应一个$z$。对于短文本，由于词少，z-->theta这一步的统计可能不具备统计意义。因为每个文本单独对应于$\theta$，所以增加文本数量不能克服这种短文本带来的缺陷。

传统的解决之道有两个。
* 是将多个短文本聚合成一个长文本。比如LDA-U将一个user的多个发言聚合在一起减少稀疏影响。但这种做法与应用相关，需要有外部信息来辅助找到聚合的单位。
* 利用一个长文档文本集作为辅助数据，将长文本的一些信息先以某种形式抽取反应到先验中，从而间接地提供给短文本。看似很美好，问题在于，与transfer learning类似，这个知识能不能传还两说，得先找到一个总体主题分布类似的长文档集。

总体来说,LDA并不适用于短文本，LDA-U略好，但是基于user的聚合受限于应用和领域知识。

### Mixture of unigram
mixture of unigram在这里表现比LDA系列更好，原因是它对于所有文档用同一个theta，克服了短文本稀疏性问题。这个模型的问题是，它假设了整个文档对应于一个z，这比较粗糙和strong了，会导致一个topic类内距离很大，因为对于长文本，较远的段落之间可能语义差异很大了。在长文本以及K较大的时候，这个模型效果会比较差，但在短文本上，这个缺陷倒是不那么明显，因此效果好于LDA系列

### BTM模型
![@BTM](https://cwlseu.github.io/images/btm/BTM-ARCH.png)
可以认为BTM综合了上述二者的优点。BTM跟mix一样，利用了整个文本集合来估计一个theta，解决了稀疏问题（我们通常有海量数据）。放宽了mix中对整个doc必须同属于一个z的约束（相当于从整doc放松到了窗口长度类的两个词），加强了LDA中每个词对应于一个Z的假设（BTM中约束了窗长内的两个词组成一个biterm对应于一个z）。这个假设很接近于人类认知，因为我们知道，通常在较短的一段文本内，topic变化不大。

说一句题外话，前几天听了一个知识图谱的讲座。他们也是用一个共现词对来排歧作为LDA的补充。比如 李娜+青藏高原 这个词对，可以很大程度上将李娜和其他人（比如另一个做音乐的李娜，无法被LDA所区分）区分开来。unigram是一个极端，一大堆词是另一个极端了。BTM相当于是这种思路在TM系列模型中的一个探索。从另一个角度，2gram或许也可以替换term作为LDA的单元，而BTM的尝试是将连续的2gram放开，允许中间有一个窗长。总的来说，这是一个相当有意思的思路，而且非常nature。

总体上看来，LDA是一个三层结构，词-> topic->doc的结构，结构层次比较明显，通过生成文档的最大概率来确定文章主题。BTM模型是可以看做是基于词对的主题模型，虽然我们实践中每个文档还是有几个离散的词汇的，但是文档的概念几乎没有了。通过最大化在不同的topic中提取词对的概率，实现对短文本的主体分布预测。

下载相关分享ppt:[从掷骰子到视频主题模型](https://deepindeed.cn/pdfs/shared_btm-caowenlong.pdf)

## 参考文献

[1]. Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2013. A biterm topic model for short texts. In Proceedings of the 22nd international conference on World Wide Web (WWW '13). ACM,
[2]. New York, NY, USA, 1445-1456. DOI: https://doi.org/10.1145/2488388.2488514
[3]. [BTM 代码](https://github.com/xiaohuiyan/BTM)
[4]. D. Mimno, H. Wallach, E. Talley, M. Leenders, and A. McCallum. Optimizing semantic coherence in topic models.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 262–272. Association for Computational Linguistics, 2011.
[5]. 欢迎留言：http://km.oa.com/group/1397/articles/show/314103
[6]. http://blog.csdn.net/pipisorry/article/details/42560693
[7]. GloVe: Global Vectors forWord Representation

---
layout: post
title: 自然语言导论
categories: [blog ]
tags: [机器学习]
description: 自然语言处理需要大量语料进行学习，而语料的集合往往被称为语料库。
---
{:toc}
- 声明：本博客欢迎转发，但请保留原作者信息!
- 作者: [曹文龙]
- 博客： <https://cwlseu.github.io/>


## 语料库
@(自然语言处理)[自然语言处理|信息检索]

## 基本概念
### 语言数据库或知识库：

1. **大规模语言数据**
* 模型参数训练
* 评测标准
2. **NLP中知识库**
* 词汇语义库
* 词法、句法规则库
* 常识库等等**

### 国内语料库现状

| 单位   |    名称   | 规模   |
|:------|:-------|:------|
|武汉大学|现代文学作品语料库|1979， 527万|
|北航|现代汉语语料库|1983， 2000万|
|北师大|中学语文教材语料库|1983,106万|
|北京语言学院|现代汉语词频统计语料库|1983,182万|
|国家语言文字工作委员会|国家级大型汉语语料库|7000万|
|清华大学|汉语歧义切分语料库|1998年,1亿|
此外，[北京大学计算语言学研究所](http://icl.pku.edu.cn/)在俞士汶教授领导下建立的综合型语言知识库(简称CLKB)涵盖了词、词组、句子、篇章各单位和词法、句法、语义各层面，从汉语向多语言辐射，从通用领域深入到专业领域。 CLKB是目前国际上规模最大且获得广泛认可的汉语语言知识资源。


## Question
### 平衡语料库

* 各分布点所选取语料量的科学依据是什么？
* 使用度是否真实地反映了语言的使用情况？

## 语料库构建中需要考虑的问题

* 静态和动态
* 代表性和平衡性
* 规模
* 语料库的管理和维护

## 语料库如何评测可用性呢？

[语言专家评测？那么专家如何平衡？]( http://www.docin.com/p-402755901.html)

# 语言模型

@(自然语言处理)[自然语言处理|信息检索]

## 基本概念

语料库和**统计方法**的成功使用。
基于大规模语料库和统计方法，可以
－ 发现语言使用的普遍规律
－ 进行机器学习、自动获取语言知识
－ 对未知语言现象进行推测
**统计基元** 
**历史**

使用所有的历史信息，简化为使用n元文法

## n-gram模型
### 定义

1-gram第i位的基元$w_i$独立于历史
2-gram被称为1阶马尔科夫链
3-gram被称为2阶马尔科夫链
例如：
$$p(s) = \prod_{i=1}^{m+1}p(w_i|w^{i-1}_{i-n+1})$$
$$\sum_{i=1}^n p(s) =1$$

### 如何获得n-gram模型

#### 参数估计

**训练语料**：
**最大似然估计**

## 数据匮乏导致零概率问题的处理方式-数据平滑(data smoothing)

调整最大似然估计的概率值,使零概率增值，使非零概率下调， “劫富济贫” ，消除零概率，改进模型的整体正确率。

### Objection

测试样本的语言模型困惑度越小越好。
好，下面对困惑度进行介绍：
对于一个平滑的n-gram，概率$p(w_i|w^{i-1}_{i-n+1})$,那么每个句子出现的概率为$p(s) = \prod_{i=1}^{m+1}p(w_i|w^{i-1}_{i-n+1})$，现在假定我们的训练语料T由$l_T$个句子构成，$(t_1,....,t_{l_T})$,那么整个测试集的概率为： 
$$p(T) = \prod_{i=1}^{l_T}p(t_i)$$
模型$p(w_i|w^{i-1}_{i-n+1})$ 对于测试语料的交叉熵为：
$$H_p(T) = -\frac{1}{W_T}log_2p(T)$$其中$W_T$是测试文本T的词数，那么模型p的困惑度$PP_p(T) = 2^{H_p(T)}$

### 数据平滑方法

#### 加1法

$$p(w_i|w_{i-1}) = \frac{1 + c(w_{i-1}w_i)}{\sum_{w_i}[1 + c(w_{i-1}w_i)]} = \frac{1 + c(w_{i-1}w_i)}{|V| + sum_{w_i}c(w_{i-1}w_i)} $$

#### 减值法/折扣法

1. Good-Turing估计

压缩比例，使得$\frac{n_1}{N}$平均分配给0概率事件。然后再进行归一化就可以了。
$$p_r = \frac{p_r}{\sum_rp_r}$$
[证明和推导参考](A. Nadas. on Turing’s Formula for Word Probabilities. In IEEE Trans. on ASSP-33, Dec. 1985. Pages 1414-1416)

2. Back-off(后备/后退)方法
对于每个计数 r > 0 的n元文法的出现次数减值，把因减值而节省下来的剩余概率根据低阶的 (n-1)gram 分配给未见事件。

3. 绝对减值法

从每个计数 r 中减去同样的量，剩余的概率量由未见事件均分。
[论文参考](H. Ney and U. Essen. Estimating Small Probabilities by Leaving-one-Out. In Proc. Eurospeech’1993. Pages 2239-2242)

4. 线性减值法

从每个计数 r 中减去与该计数成正比的量(减值函数为线性的)，剩余概率量 $\alpha$ 被$n_0$个未见事件均分。

### 删除插值法

用低阶语法估计高阶语法，即当 3-gram的值不能从训练数据中准确估计时，用 2-gram 来替代，同样，当 2-gram 的值不能从训练语料中准确估计时，可以用 1-gram 的值来代替。

### 数据平滑方法详细介绍与比较
http://www-2.cs.cmu.edu/~sfc/html/publications.html

## 自然语言发展历程

1. 50-60年代，最为人工智能领域的应用主要是机器翻译方面尤其是60年代。普遍采用**基于原则**的方法。
2. 90年代，大规模词典和真是语料库的研制，基于语料库的**统计**自然语言成为重要方法。[因此学习统计分析是是多么重要啊]
3. 过去20年，随着互联网的普及，为自然语言处理领域提供了强有力的应用牵引和海量的语言资源。自然语言处理和信息检索系统结合。统计自然语言受限于训练集的规模，过拟合问题严重，推广能力不足。
4. 近几年，**深度学习**方法，基于分布学习的**词义**和**语义**很好地效果。Web2.0积累了大量的User Generated Content.为自然语言提供了新的资源和技术创新的源泉。基于知识和基于统计的方法融合受到关注。

## 信息抽取

系统并不要求能够对自然语言文本进行深层理解，而是从中抽取有用信息，作为自然语言部分理解的一种形式。在过载的信息中，快速准确获取信息的技术手段。

##定义

1997：从自然语言文本中抽取指定类型的***实体***、***文本***、***关系***和***事件***等事实信息。

## 评测标准

1. MUC （message understanding conferences）
    实体识别，共指消解，模板关系抽取等
2. ACE  automatic content extraction
    2009年变名为TAC ( Text analysis conference )
    关系抽取，事件抽取
3. TAC-KBP ( Knowledge Base Population )
    实体连接 属性抽取

## 命名实体识别

识别1. 人名  2. 机构名 3. 地名 4. 时间 5. 日期 6. 货币 7. 百分比

###注意问题

* 人名地名机构名识别难度大，
* 上下文密切，不同而实体在不同语义下具有不同的实体类型，如：新世界

Wu EMNLP 2005

### 主要方法

1. 通过分析种子实体在查询中的上下文，利用模板找到同类别的实例。
2. 构造向量，计算
Ref： Wang ICDM 2007

### 系统框架
    
    爬取模块 --> 抽取模块 --> 排序模块

### 评价指标
使用$MAP$ 进行评测

## 实体消歧
### 定义
一个实体指称项对应多个真实世界的实体。确定一个实体指称对应真实世界的什么实体。
### 常见方法
基于聚类
基于链接

## 基于聚类消歧方法

1. 同一指称项具有近似的上下文
2. 利用聚类算法尽心小气

###关键问题

选取那些特征対指称项进行表示

### 词袋模型

* 利用待消歧词的实体周边的词进行构造向量
* 利用空间模型来计算两种实体指称项的相似度进行聚类
* 没有考虑词的语义信息

###语义特征

* 利用**SVD**挖掘语义特征

### 社会化网络

不同的人具有不同的社会，通过**社会网络关系**挖掘进行消除歧义

####维基百科的知识[Han ]

利用实体上下文的维基百科条目对尸体进行向量表示
利用维基百科条目之间的相似度进行计算指称之间的相似度（解决数据稀疏的问题）

### 多源异构知识[Han ACL 2010]

仅仅使用wikimadia是有限的，通过结合多种知识库，形成语义图进行知识挖掘。

### 实体消歧：评测-WePS

###挑战

消歧目标难以确定
缺乏实体的显示表示

## 基于链接的消歧

* 候选实体发现
* 候选实体的链接
候选实体发现
1. 利用wikipedia发现实体
2. 利用上下文获取缩略语候选实体

### 候选实体链接

### 类别特征[Bunescu EACL 2006]
1. 实体流行度等特征
2. 传统的方法仅仅是计算实体指称项图实体的相似度，未考虑实体的背景，先验知识等问题。

### 结构化数据中的实体链接 [Shen SIGKDD 2012]
### 社交数据中的实体链接[Shen SIGKDD 2013]
### 评测标准-TAC-KBP
### 总结

实体链接方法主要是如何更有效挖掘实体指称项信息，如何更准确地计算湿体质表象和实体概念之间的相似度

## 实体间关系抽取
### 定义

自动识别由一堆概念和联系这对概念的关系构成的相关三元组

###非结构化关系抽取
##传统关系抽取
###基于特征向量：最大熵 和支持向量机

获取有效此词法句法语义特征

###基于核函数：浅层树核和依存树核 最短依存树核等等

挖掘反应语义关系的结构化信息和计算结构化信息之间的相似度

###基于神经网络

如何设计合理的网络结构，从而捕获更多的信息，进而更准确地完成关系的抽取
基于卷积神经网络的关系抽取
判断句子中实体之间的语义关系

#### 传统方法问题

1. 错误累计
2. 人工设计特征
通过CNN学习文本语义特征
不需要人工设计特征

## 开放域关系抽取
### 按需抽取BoostStrapping
    模板生成--> 实体抽取 --> 
<!--Turing Center Machine Reading-->
1. 开放域关系抽取：从NYT中抽取FreeBase的关系类别（Zeng EMNLP 2015）
2. 基于细粒度实体类型特征发现的弱监督关系抽取Liu Coling 2014

## 开放关系的发现
关系发现就是利用知识图谱中现有的知识推断未知的知识，就是链接预测

### 归纳逻辑编程

符合逻辑宝石精确，表达能力强
但是很难在大规模语料库上进行推广。

### 概率图模型

-- 马尔科夫随机场
-- 概率软逻辑

## 机器翻译的产生

1949年，W. Weaver提出“机器翻译”：

    翻译类似于解读密码的过程：当我阅读一篇用俄语写的文章时， 我可以说这篇文章实际上是用英文写的， 只不过它用另外一种奇怪的符号编了码， 当我阅读时， 我是在进行解码。 --1949

## 机器翻译的现状

1990年左右，统计机器翻译由于仅依赖双语平行语料，大大降低了进入机器翻译研究的门槛；
2014年，深度学习应用于机器翻译中，获得performance的提升。

## MT的主要困难

* 句法结构歧义，词汇歧义，新的词汇等未知现象
* 不仅仅是翻译，还有不同语言之间的文化差异
* 翻译结果不唯一

## 基本翻译方法
### 直接转换法

从源语言句子的表层出发，将单词、短语或句子直接置换成目标语言译文，必要时进行简单的词序调整。对原文句子的分析仅满足于特定译文生成的需要。这类翻译系统一般针对某一个特定的语言对，将分析与生成、语言数据、文法和规则与程序等都融合在一起。
例如：
I like Mary. => Me(I) gusta(like) Maria(Mary).
X like Y => Y X gusta

### 基于规则的翻译方法

对源语言和目标语言均进行适当描述、 把翻译机制与语法分开、 用规则描述语法的实现思想， 这就是基于规则的翻译方法。“独立分析－独立生成－相关转换”：
* 对源语言句子进行词法分析
* 对源语言句子进行句法/语义分析
* 源语言句子结构到译文结构的转换
* 译文句法结构生成
* 源语言词汇到译文词汇的转换
* 译文词法选择与生成
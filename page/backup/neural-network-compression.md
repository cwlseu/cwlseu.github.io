---
layout: post
title: 神经网络模型压缩与加速
tags: [计算机视觉, 神经网络压缩] 
categories: [blog ]
notebook: 视觉算法
---

## 引言

当前人工智能在图像、语音等领域取得的成功很大程度上归功于大型多层的深度神经网络模型。为了达到更好的效果或解决更复杂的问题，这些模型还在日益变大、变复杂。然而，在人工智能的多数应用领域，例如机器翻译、语音识别、自动驾驶等等，用户对人工智能系统的响应速度都非常敏感，有些甚至关乎生命安全。因此深度神经网络的低延迟推理是人工智能应用落地场景中一个非常关键的问题。

为了解决计算需求不断增长的问题，学术界和工业界都进行了大量的努力。一个直接的方法是通过定制化专用计算硬件（例如GPU、FPGA、ASIC），通过对深度神经网络进行领域特定体系结构设计（domain specific architecture design）提升神经网络的计算效率和速度，例如GPU中的Tensor Cores和 TPU中基于脉动阵列的矩阵乘法单元。然而，这种方法仍然满足不了日益增长的需求，深度学习模型还在增大，同时对于很多部署在移动端的AI应用，因为受到功耗和电源供应的限制，移动端处理器计算力的增长更是相对缓慢。


## 模型压缩 | 结构性剪枝
https://zhuanlan.zhihu.com/p/48269250
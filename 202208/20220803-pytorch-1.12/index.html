<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7CTimes+New+Roman:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.deepindeed.cn","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":28},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="近日，PyTorch 发布了1.12 版本，其中针对 Torchscript 部署优化做了一些改进, 并引入了 nvFuser，我们来一起看看吧。">
<meta property="og:type" content="article">
<meta property="og:title" content="认识 PyTorch 1.12 之后的 nvFuser">
<meta property="og:url" content="http://www.deepindeed.cn/202208/20220803-pytorch-1.12/index.html">
<meta property="og:site_name" content="Deepindeed">
<meta property="og:description" content="近日，PyTorch 发布了1.12 版本，其中针对 Torchscript 部署优化做了一些改进, 并引入了 nvFuser，我们来一起看看吧。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pytorch.org/tutorials/_images/nvfuser_tutorial_6.png">
<meta property="article:published_time" content="2022-08-03T12:59:39.000Z">
<meta property="article:modified_time" content="2022-12-18T10:51:59.686Z">
<meta property="article:author" content="CharlesCao">
<meta property="article:tag" content="Backends">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pytorch.org/tutorials/_images/nvfuser_tutorial_6.png">


<link rel="canonical" href="http://www.deepindeed.cn/202208/20220803-pytorch-1.12/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://www.deepindeed.cn/202208/20220803-pytorch-1.12/","path":"202208/20220803-pytorch-1.12/","title":"认识 PyTorch 1.12 之后的 nvFuser"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>认识 PyTorch 1.12 之后的 nvFuser | Deepindeed</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-86501439-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-86501439-1","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>




<link rel="dns-prefetch" href="https://waline.vercel.app"><link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Deepindeed</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">心有猛虎，细嗅蔷薇</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-tools"><a href="/tools/" rel="section"><i class="fa fa-globe fa-fw"></i>tools</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#nvfuser-%E6%8A%80%E6%9C%AF"><span class="nav-number">1.</span> <span class="nav-text">NVFuser 技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88%E6%98%AF%E5%9F%BA%E6%9C%AC%E7%9A%84%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-number">1.1.</span> <span class="nav-text">算子融合是基本的优化策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nvfuser-in-pytorch"><span class="nav-number">1.2.</span> <span class="nav-text">nvFuser in PyTorch</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nvfuser-%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">NVFuser 的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6nvfuser-%E5%BC%80%E5%85%B3"><span class="nav-number">2.1.</span> <span class="nav-text">控制nvFuser 开关</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nvfusion-%E7%9A%84%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="nav-number">3.</span> <span class="nav-text">NVFusion 的性能评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#timm-%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">TIMM 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#functorch-memory_efficient_fusion"><span class="nav-number">3.2.</span> <span class="nav-text">FuncTorch:
memory_efficient_fusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fuser%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="nav-number">3.3.</span> <span class="nav-text">fuser上下文</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">4.</span> <span class="nav-text">参考链接</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="CharlesCao"
      src="/images/bird.png">
  <p class="site-author-name" itemprop="name">CharlesCao</p>
  <div class="site-description" itemprop="description">In me the tiger sniffs the rose.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="mailto:caowenlong92@gmail.com" title="E-Mail → mailto:caowenlong92@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5221628" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5221628" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cwlseu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;cwlseu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://pytorch.org/" title="https:&#x2F;&#x2F;pytorch.org" rel="noopener" target="_blank">Pytorch</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://cplusplus.com/reference" title="https:&#x2F;&#x2F;cplusplus.com&#x2F;reference" rel="noopener" target="_blank">CppReference</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://docs.nvidia.com/cuda/index.html" title="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;index.html" rel="noopener" target="_blank">NVIDIA CUDA</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/cwlseu" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.deepindeed.cn/202208/20220803-pytorch-1.12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/bird.png">
      <meta itemprop="name" content="CharlesCao">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Deepindeed">
      <meta itemprop="description" content="In me the tiger sniffs the rose.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="认识 PyTorch 1.12 之后的 nvFuser | Deepindeed">
      <meta itemprop="description" content="近日，PyTorch 发布了1.12 版本，其中针对 Torchscript 部署优化做了一些改进, 并引入了 nvFuser，我们来一起看看吧。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          认识 PyTorch 1.12 之后的 nvFuser
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-08-03 20:59:39" itemprop="dateCreated datePublished" datetime="2022-08-03T20:59:39+08:00">2022-08-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-12-18 18:51:59" itemprop="dateModified" datetime="2022-12-18T18:51:59+08:00">2022-12-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/blog/" itemprop="url" rel="index"><span itemprop="name">blog</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">评论数：</span>
  
    <a title="waline" href="/202208/20220803-pytorch-1.12/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/202208/20220803-pytorch-1.12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>21 分钟</span>
    </span>
</div>

            <div class="post-description">近日，PyTorch 发布了1.12 版本，其中针对 Torchscript 部署优化做了一些改进, 并引入了 nvFuser，我们来一起看看吧。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="nvfuser-技术">NVFuser 技术</h2>
<p>nvFuser 是一种深度学习编译器，可快速灵活的即时编译（JIT，
Just-in-time） GPU
特定代码，以可靠地自动加速用户的网络，通过在运行时生成快速自定义“融合”内核，为在
Volta 和后来的 CUDA 加速器上运行的深度学习网络提供加速。nvFuser 专为满足
PyTorch
社区的独特需求而设计，它支持各种网络架构和程序，支持动态输入(不同shape,
strides)。nvFuser 基于 Torchscript
对计算图进行优化和加速。我们一般是需要将PyTorch
Eager模式的动态图模型转化为 Torchscript IR表示的计算图，然后再让nvFuser
在特定 GPU 设备上优化模型。 该技术从 PyTorch 1.12 全面引入。</p>
<h3 id="算子融合是基本的优化策略">算子融合是基本的优化策略</h3>
<ul>
<li>计算少， memory moving 是非常耗时的</li>
<li>Fusion is primarly the optimization of keeping intermediate values
in cache or registers</li>
<li>Fusion is user defined operations -&gt; efficient device specific
code</li>
</ul>
<p>nvFuser 当前支持什么?</p>
<ul>
<li>Backward pass</li>
<li>bool, int32, int64, fp16, bfloat16, fp32, fp64</li>
<li>pointwise ops, reductions, normalizations, view dynamic shape</li>
<li>Coming soon: Channels last(performance), Complex, Transpose, Pooling
Layers, Mattel</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/cwlseu/deepindeed_repo@main/images/image-20221218180556966.png"
alt="GPU" />
<figcaption aria-hidden="true">GPU</figcaption>
</figure>
<p>Memory Efficiency: Assumes an op can be done in a single function and
fits into cache. This is not always possible.</p>
<p>nvFuser 提升推理过程Memory Efficiency， 大约提升10%+</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/cwlseu/deepindeed_repo@main/images/image-20221218180952053.png"
alt="nvFuser对显存利用率的影响" />
<figcaption aria-hidden="true">nvFuser对显存利用率的影响</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/cwlseu/deepindeed_repo@main/images/image-20221218181258017.png"
alt="nvFuser对带宽的影响" />
<figcaption aria-hidden="true">nvFuser对带宽的影响</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41958/">The
Next Generation of GPU Performance in PyTorch with nvFuser</a></p>
<h3 id="nvfuser-in-pytorch">nvFuser in PyTorch</h3>
<p>当前主要有三种机制可以将 PyTorch 模型进行 capture, translate and
pass，来输出可以让nvFuser的程序：</p>
<ul>
<li><p>Torchscript.jit.script</p></li>
<li><p>FuncTorch</p>
<p>该体系不直接查看用户 Python 脚本，而是在运行时通过插入对 PyTorch
Op状态的capture机制。 我们将这种类型的捕获系统称为“跟踪程序获取 （trace
program acquisition）”，因为我们正在跟踪已执行的操作。 FuncTorch
不执行自己的自动微分——它只是直接跟踪 PyTorch 的 autograd
以获得向后图。</p></li>
<li><p>TorchDynamo</p>
<p>TorchDynamo 是另一种建立在 FuncTorch 之上的程序获取机制。 TorchDynamo
解析从用户脚本生成的 Python 字节码，以便选择要使用 FuncTorch
跟踪的部分。 TorchDynamo
的好处是它能够将装饰器应用于用户的脚本，有效地隔离应该发送给 FuncTorch
的内容，使 FuncTorch 更容易成功地跟踪复杂的 Python 脚本。</p></li>
</ul>
<p>上面这三类机制可供用户直接交互，而 nvFuser
会自动无缝地优化用户代码的性能关键区域。
这些系统自动将解析后的用户程序发送到 nvFuser，以便 nvFuser 可以：</p>
<ol type="1">
<li><p>分析在 GPU 上运行的操作</p></li>
<li><p>为这些操作规划并行化和优化策略</p></li>
<li><p>在生成的 GPU 代码中应用这些策略</p></li>
<li><p>Runtime-编译生成的优化 GPU 函数</p></li>
<li><p>在后续迭代中执行那些 CUDA 内核</p></li>
</ol>
<p>重要的是要注意 nvFuser 尚不支持所有 PyTorch Op，并且在此处讨论的
nvFuser 中仍有一些正在积极改进的场景。 然而，nvFuser 今天确实支持许多 DL
性能关键 Op，并且支持的 Op 数量将在后续的 PyTorch 版本中增加。 nvFuser
能够为其支持的操作生成高度专业化和优化的 GPU 函数。 这意味着 nvFuser
能够为 TorchDynamo 和 FuncTorch 等新的 PyTorch 体系提供动力，以将
PyTorch 灵活性与无与伦比的性能相结合。</p>
<h2 id="nvfuser-的应用">NVFuser 的应用</h2>
<h3 id="控制nvfuser-开关">控制nvFuser 开关</h3>
<ol type="1">
<li>Allow single node fusion
<code>torch._C._jit_set_nvfuser_single_node_mode(True)</code> Fusion
group is only created when two or more compatible ops are grouped
together. Turn on single node fusion would allow fusion pass to create
fusion group with a single node, this is very handy for testing and
could be useful when single node generated kernel out-performs native
cuda kernels in framework.</li>
<li>Allow horizontal fusion
<code>torch._C._jit_set_nvfuser_horizontal_mode(True)</code> Fusion pass
fuses producer to consumer, horizontal mode allows sibling nodes that
shared tensor input to be fused together. This could save input memory
bandwidth.</li>
<li>Turn off guard for fusion
<code>torch._C._jit_set_nvfuser_guard_mode(False)</code> This disables
the runtime check on fusion group pre-assumptions (tensor meta
information / constant inputs / profiled constants), this really is only
used for testing as we want to ensure generated kernels are indeed
tested and you should avoid using this in training scripts.</li>
<li>Turn off fusion for certain node kinds
<code>torch._C._jit_set_nvfuser_skip_node_kind("aten::add", True)</code>
This disables fusion for certain nodes, but allows other nodes to
continue being fused. The first parameter is the node kind, and the
second parameter is whether to toggle the node on or off in fusion.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    o = x + <span class="number">1.0</span></span><br><span class="line">    o = o.relu()</span><br><span class="line">    <span class="keyword">return</span> o</span><br><span class="line">shape = (<span class="number">2</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.rand(*shape).cuda()</span><br><span class="line">t = torch.jit.script(forward)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.jit.fuser(<span class="string">&quot;fuser2&quot;</span>):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        o = t(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<p><code>PYTORCH_JIT_LOG_LEVEL="profiling_graph_executor_impl" python &lt;your pytorch script&gt;</code></p>
<p>打印出的计算图很简单，您应该寻找 <code>prim::CudaFusionGroup_X</code>
以获取融合内核。 虽然分析执行器会转储很多东西，但最重要的部分是优化图。
在这个例子中，它显示了一个融合组，这表明融合正在发生，你应该期待融合内核！</p>
<p>请注意，在进行训练时,autodiff 可能会阻止 Op 融合 。 Fusion pass 仅在
prim::DifferentiableGraph
中运行，因此你应该首先检查目标操作是否在可微分图子图中。 计算图 Dump
后看起来非常混乱，因为它直接 dump
所有由分析执行器执行的计算图，而可微分计算图是通过嵌套计算图执行器执行的。
因此，对于每个模型，你可能会看到一些分段优化的子图，其中每个子图都对应于原始图中的一个可微分节点。</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Before Fusion:</span><br><span class="line">graph(%x.1 : Tensor):</span><br><span class="line">  %1 : int = prim::Constant[value=1]()</span><br><span class="line">  %2 : float = prim::Constant[value=1.]() # test/test_nvfuser.py:4:12</span><br><span class="line">  %5 : Tensor = prim::profile[profiled_type=Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0), seen_none=0](%x.1)</span><br><span class="line">  %o.1 : Tensor = aten::add(%5, %2, %1) # test/test_nvfuser.py:4:8</span><br><span class="line">  %6 : Tensor = prim::profile[profiled_type=Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0), seen_none=0](%o.1)</span><br><span class="line">  %o.5 : Tensor = aten::relu(%6) # test/test_nvfuser.py:5:8</span><br><span class="line">  %7 : Tensor = prim::profile[profiled_type=Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0), seen_none=0](%o.5)</span><br><span class="line">   = prim::profile()</span><br><span class="line">  return (%7)</span><br><span class="line"></span><br><span class="line">Optimized Graph:</span><br><span class="line">graph(%x.1 : Tensor):</span><br><span class="line">  %1 : bool = prim::CudaFusionGuard[types=[Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)]](%x.1)</span><br><span class="line">  %2 : Tensor = prim::If(%1)</span><br><span class="line">    block0():</span><br><span class="line">      %o.8 : Tensor = prim::CudaFusionGroup_0[cache_id=0](%x.1)</span><br><span class="line">      -&gt; (%o.8)</span><br><span class="line">    block1():</span><br><span class="line">      %4 : Function = prim::Constant[name=&quot;fallback_function&quot;, fallback=1]()</span><br><span class="line">      %5 : (Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)) = prim::CallFunction(%4, %x.1)</span><br><span class="line">      %6 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = prim::TupleUnpack(%5)</span><br><span class="line">      -&gt; (%6)</span><br><span class="line">  return (%2)</span><br><span class="line"></span><br><span class="line">with prim::CudaFusionGroup_0 = graph(%0 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)):</span><br><span class="line">  %1 : int = prim::Constant[value=1]()</span><br><span class="line">  %2 : float = prim::Constant[value=1.]() # test/test_nvfuser.py:4:12</span><br><span class="line">  %o.1 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = aten::add(%0, %2, %1) # test/test_nvfuser.py:4:8</span><br><span class="line">  %o.5 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = aten::relu(%o.1) # test/test_nvfuser.py:5:8</span><br><span class="line">  return (%o.5)</span><br></pre></td></tr></table></figure>
<p>Cuda fusion dump 将输入和输出图提供给 fusion pass。
这种方式可以用来检查融合传递逻辑。</p>
<p><code>PYTORCH_JIT_LOG_LEVEL="graph_fuser" python  &lt;script_name&gt;</code></p>
<p>我们从结果中可以看到有两个计算图:</p>
<ul>
<li>Fusion pass 执行之前对应的计算图</li>
<li>Fusion pass之后的计算图，其中包括两个 CudaFusionGroup，每个
CudaFusionGroup 将触发代码生成系统去生成 Kernel 来执行子图。</li>
</ul>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[DUMP graph_fuser.cpp:2352] Before Fusion: </span><br><span class="line">[DUMP graph_fuser.cpp:2352] graph(%x.1 : Tensor):</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %2 : float = prim::Constant[value=1.]() # test/test_nvfuser.py:4:12</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %1 : int = prim::Constant[value=1]()</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %3 : Tensor = prim::profile[profiled_type=Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0), seen_none=0](%x.1)</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %o.1 : Tensor = aten::add(%3, %2, %1) # test/test_nvfuser.py:4:8</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %5 : Tensor = prim::profile[profiled_type=Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0), seen_none=0](%o.1)</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %o.5 : Tensor = aten::relu(%5) # test/test_nvfuser.py:5:8</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   %7 : Tensor = prim::profile[profiled_type=Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0), seen_none=0](%o.5)</span><br><span class="line">[DUMP graph_fuser.cpp:2352]   return (%7)</span><br><span class="line">[DUMP graph_fuser.cpp:2432] Before Compilation: </span><br><span class="line">[DUMP graph_fuser.cpp:2432] graph(%x.1 : Tensor):</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %12 : bool = prim::CudaFusionGuard[types=[Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)]](%x.1)</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %11 : Tensor = prim::If(%12)</span><br><span class="line">[DUMP graph_fuser.cpp:2432]     block0():</span><br><span class="line">[DUMP graph_fuser.cpp:2432]       %o.8 : Tensor = prim::CudaFusionGroup_0(%x.1)</span><br><span class="line">[DUMP graph_fuser.cpp:2432]       -&gt; (%o.8)</span><br><span class="line">[DUMP graph_fuser.cpp:2432]     block1():</span><br><span class="line">[DUMP graph_fuser.cpp:2432]       %o.5 : Tensor = prim::FallbackGraph_1(%x.1)</span><br><span class="line">[DUMP graph_fuser.cpp:2432]       -&gt; (%o.5)</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   return (%11)</span><br><span class="line">[DUMP graph_fuser.cpp:2432] with prim::CudaFusionGroup_0 = graph(%2 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)):</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %4 : int = prim::Constant[value=1]()</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %3 : float = prim::Constant[value=1.]() # test/test_nvfuser.py:4:12</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %o.1 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = aten::add(%2, %3, %4) # test/test_nvfuser.py:4:8</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %o.5 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = aten::relu(%o.1) # test/test_nvfuser.py:5:8</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   return (%o.5)</span><br><span class="line">[DUMP graph_fuser.cpp:2432] with prim::FallbackGraph_1 = graph(%x.1 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)):</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %1 : int = prim::Constant[value=1]()</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %2 : float = prim::Constant[value=1.]() # test/test_nvfuser.py:4:12</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %o.1 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = aten::add(%x.1, %2, %1) # test/test_nvfuser.py:4:8</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   %o.5 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = aten::relu(%o.1) # test/test_nvfuser.py:5:8</span><br><span class="line">[DUMP graph_fuser.cpp:2432]   return (%o.5)</span><br></pre></td></tr></table></figure>
<h2 id="nvfusion-的性能评估">NVFusion 的性能评估</h2>
<h3 id="timm-模型">TIMM 模型</h3>
<p><img
src="https://pytorch.org/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-2.png" /></p>
<p>nvFuser, can also significantly reduce the training time of TIMM
networks, up to over <strong>1.3x </strong> vs. eager PyTorch, and up to
<strong>1.44x</strong> vs. eager PyTorch when combined with the
torch.amp module. Figure 1 shows nvFuser’s speedup without torch.amp,
and when torch.amp is used with the NHWC (“channels last”) and NCHW
(“channels first”) formats. nvFuser is integrated in TIMM through
FuncTorch tracing directly (without TorchDynamo) and can be used by
adding the <a
target="_blank" rel="noopener" href="https://github.com/rwightman/pytorch-image-models/commit/ca991c1fa57373286b9876aa63370fd19f5d6032">–aot-autograd
command line argument</a> when running the TIMM benchmark or training
script.</p>
<h3 id="functorch-memory_efficient_fusion">FuncTorch:
memory_efficient_fusion</h3>
<ul>
<li>Eager Mode - Primitive Definition: Average iterations per second:
591.93</li>
<li>TorchScript - Primitive definition: Average iterations per second:
1657.58</li>
<li>FuncTorch - Primitive definition: Average iterations per second:
2918.51</li>
</ul>
<figure>
<img src="https://pytorch.org/tutorials/_images/nvfuser_tutorial_6.png"
alt="use memory efficient fusion" />
<figcaption aria-hidden="true">use memory efficient fusion</figcaption>
</figure>
<p>从结果中来看，新版本中的nvFuser 方法比原来的
Torchscript方式性能进一步提升。</p>
<h3 id="fuser上下文">fuser上下文</h3>
<p>一行代码，在 efficient net-b3 中的性能提升情况。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># define EfficientNet-b3 model or load from torchvision.models</span></span><br><span class="line">shape = (<span class="number">2</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.rand(*shape).cuda()</span><br><span class="line">t = torch.jit.script(forward)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.jit.fuser(<span class="string">&quot;fuser2&quot;</span>):</span><br><span class="line">     o = t(<span class="built_in">input</span>)</span><br><span class="line">     </span><br><span class="line"><span class="comment"># torch/jit/_fuser.py</span></span><br><span class="line"><span class="meta">@contextlib.contextmanager</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fuser</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A context manager that facilitates switching between</span></span><br><span class="line"><span class="string">    backend fusers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Valid names:</span></span><br><span class="line"><span class="string">    * ``fuser0`` - enables only legacy fuser</span></span><br><span class="line"><span class="string">    * ``fuser1`` - enables only NNC</span></span><br><span class="line"><span class="string">    * ``fuser2`` - enables only nvFuser</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    old_cpu_fuse = torch._C._jit_can_fuse_on_cpu()</span><br><span class="line">    old_gpu_fuse = torch._C._jit_can_fuse_on_gpu()</span><br><span class="line">    old_texpr_fuser_state = torch._C._jit_texpr_fuser_enabled()</span><br><span class="line">    old_nvfuser_state = torch._C._jit_nvfuser_enabled()</span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">&#x27;fuser0&#x27;</span>:  <span class="comment"># legacy fuser</span></span><br><span class="line">        torch._C._jit_override_can_fuse_on_cpu(<span class="literal">True</span>)</span><br><span class="line">        torch._C._jit_override_can_fuse_on_gpu(<span class="literal">True</span>)</span><br><span class="line">        torch._C._jit_set_texpr_fuser_enabled(<span class="literal">False</span>)</span><br><span class="line">        torch._C._jit_set_nvfuser_enabled(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">elif</span> name == <span class="string">&#x27;fuser1&#x27;</span>:  <span class="comment"># NNC</span></span><br><span class="line">        old_profiling_executor = torch._C._jit_set_profiling_executor(<span class="literal">True</span>)</span><br><span class="line">        old_profiling_mode = torch._C._jit_set_profiling_mode(<span class="literal">True</span>)</span><br><span class="line">        torch._C._jit_override_can_fuse_on_cpu(<span class="literal">False</span>)</span><br><span class="line">        torch._C._jit_override_can_fuse_on_gpu(<span class="literal">True</span>)</span><br><span class="line">        torch._C._jit_set_texpr_fuser_enabled(<span class="literal">True</span>)</span><br><span class="line">        torch._C._jit_set_nvfuser_enabled(<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">elif</span> name == <span class="string">&#x27;fuser2&#x27;</span>:  <span class="comment"># nvFuser</span></span><br><span class="line">        torch._C._jit_override_can_fuse_on_cpu(<span class="literal">False</span>)</span><br><span class="line">        torch._C._jit_override_can_fuse_on_gpu(<span class="literal">False</span>)</span><br><span class="line">        torch._C._jit_set_texpr_fuser_enabled(<span class="literal">False</span>)</span><br><span class="line">        torch._C._jit_set_nvfuser_enabled(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;unrecognized fuser option&quot;</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">yield</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="keyword">if</span> name == <span class="string">&#x27;fuser1&#x27;</span>:  <span class="comment"># NNC</span></span><br><span class="line">            torch._C._jit_set_profiling_executor(old_profiling_executor)</span><br><span class="line">            torch._C._jit_set_profiling_mode(old_profiling_mode)</span><br><span class="line">        <span class="comment"># recover the previous values</span></span><br><span class="line">        torch._C._jit_override_can_fuse_on_cpu(old_cpu_fuse)</span><br><span class="line">        torch._C._jit_override_can_fuse_on_gpu(old_gpu_fuse)</span><br><span class="line">        torch._C._jit_set_texpr_fuser_enabled(old_texpr_fuser_state)</span><br><span class="line">        torch._C._jit_set_nvfuser_enabled(old_nvfuser_state)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 26%" />
<col style="width: 11%" />
<col style="width: 20%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Engine</th>
<th>Batch_size</th>
<th>time(ms)/per image</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>PyTorch-1.11</td>
<td>1</td>
<td>14.780</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>PyTorch-1.11</td>
<td>4</td>
<td>4.2275</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>PyTorch-1.11</td>
<td>8</td>
<td>2.0720</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>PyTorch-1.11</td>
<td>16</td>
<td>1.0915</td>
<td></td>
</tr>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>PyTorch-1.12</td>
<td>1</td>
<td>14.7769</td>
<td>PyTorch版本不同</td>
</tr>
<tr class="even">
<td></td>
<td>PyTorch-1.12</td>
<td>4</td>
<td>4.1610</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>PyTorch-1.12</td>
<td>8</td>
<td>2.0671</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>PyTorch-1.12</td>
<td>16</td>
<td>1.0677</td>
<td></td>
</tr>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>Torchscript-1.11</td>
<td>1</td>
<td>10.7</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.11</td>
<td>4</td>
<td>3.8995</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Torchscript-1.11</td>
<td>8</td>
<td>2.3749</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.11</td>
<td>16</td>
<td>1.6607</td>
<td></td>
</tr>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>Torchscript-1.12</td>
<td>1</td>
<td>9.7512</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12</td>
<td>4</td>
<td>2.9710</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Torchscript-1.12</td>
<td>8</td>
<td>1.4233</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12</td>
<td>16</td>
<td>0.7389</td>
<td>trochscript1.12性能提升30%</td>
</tr>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>Torchscript-1.12-fuser0</td>
<td>1</td>
<td>7.8338</td>
<td>legacy fuser</td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12-fuser0</td>
<td>4</td>
<td>2.1315</td>
<td>legacy fuser</td>
</tr>
<tr class="odd">
<td></td>
<td>Torchscript-1.12-fuser0</td>
<td>8</td>
<td>1.1117</td>
<td>legacy fuser</td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12-fuser0</td>
<td>16</td>
<td>0.5922</td>
<td>legacy fuser</td>
</tr>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>Torchscript-1.12-fuser1</td>
<td>1</td>
<td>6.7735</td>
<td>NNC</td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12-fuser1</td>
<td>4</td>
<td>1.8711</td>
<td>NNC</td>
</tr>
<tr class="odd">
<td></td>
<td>Torchscript-1.12-fuser1</td>
<td>8</td>
<td>0.9110</td>
<td>NNC</td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12-fuser1</td>
<td>16</td>
<td>0.5894</td>
<td>NNC</td>
</tr>
<tr class="odd">
<td>EfficientNet-b3_224</td>
<td>Torchscript-1.12-fuser2</td>
<td>1</td>
<td>7.9868</td>
<td>NVFuser</td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12-fuser2</td>
<td>4</td>
<td>2.2119</td>
<td>NVFuser</td>
</tr>
<tr class="odd">
<td></td>
<td>Torchscript-1.12-fuser2</td>
<td>8</td>
<td>1.0809</td>
<td>NVFuser</td>
</tr>
<tr class="even">
<td></td>
<td>Torchscript-1.12-fuser2</td>
<td>16</td>
<td>0.5914</td>
<td>NVFuser</td>
</tr>
</tbody>
</table>
<h2 id="参考链接">参考链接</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://pytorch.org/blog/pytorch-1.12-released/">https://pytorch.org/blog/pytorch-1.12-released/</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/tree/release/1.12/torch/csrc/jit/codegen/cuda">https://github.com/pytorch/pytorch/tree/release/1.12/torch/csrc/jit/codegen/cuda</a></li>
<li>https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/</li>
<li>https://pytorch.org/tutorials/intermediate/nvfuser_intro_tutorial.html</li>
<li>https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31952/</li>
</ul>

    </div>

    
    
    
      


    <footer class="post-footer"><div class="post-widgets">
    <div
      class="social-share"
      
        data-sites="weibo,qq,wechat,tencent,douban,qzone,linkedin,diandian,facebook,twitter,google"
      
      
        data-wechat-qrcode-title="share.title"
      
      
        data-wechat-qrcode-helper="share.prompt"
      
    >
    </div>
  </div>
  <script src="https://lib.baomitu.com/social-share.js/1.0.16/js/social-share.min.js"></script>
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>CharlesCao
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://www.deepindeed.cn/202208/20220803-pytorch-1.12/" title="认识 PyTorch 1.12 之后的 nvFuser">http://www.deepindeed.cn/202208/20220803-pytorch-1.12/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Backends/" rel="tag"># Backends</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/202203/20220310-nlp-text-data-augmentation/" rel="prev" title="NLP 文本场景的数据优化">
                  <i class="fa fa-chevron-left"></i> NLP 文本场景的数据优化
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/202208/20220831-basic-cuda-interface/" rel="next" title="CUDA并行编程学习笔记">
                  CUDA并行编程学习笔记 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-waline">waline</a></li>
            <li class="tab"><a href="#comment-gitalk">gitalk</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane waline" id="comment-waline">
              <div class="comments" id="waline"></div>
            </div>
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments gitalk-container"></div>
            </div>
        </div>
      </div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CharlesCao</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">510k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">14:10</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdn.jsdelivr.net/npm/pdfobject@2.2.7/pdfobject.min.js","integrity":"sha256-ph3Dk89VmuTVXG6x/RDzk53SU9LPdAh1tpv0UvnDZ2I="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdn.jsdelivr.net/npm/mermaid@9.0.1/dist/mermaid.min.js","integrity":"sha256-CemUs9ITT7liCZpVMktcEw0BpAOZ1+mujlMB3UyuImU="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://waline.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"libUrl":"https://unpkg.com/@waline/client@v2/dist/waline.js","locale":{"placeholder":"欢迎交流指正"},"emoji":["https://unpkg.com/@waline/emojis@1.0.1/weibo","https://unpkg.com/@waline/emojis@1.0.1/alus","https://unpkg.com/@waline/emojis@1.0.1/bilibili","https://unpkg.com/@waline/emojis@1.0.1/qq","https://unpkg.com/@waline/emojis@1.0.1/tieba","https://unpkg.com/@waline/emojis@1.0.1/tw-emoji"],"meta":["nick","mail"],"requiredMeta":["mail","nick","mail"],"login":"enable","el":"#waline","comment":true,"path":"/202208/20220803-pytorch-1.12/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"cwlseu","repo":"blog-comment","client_id":"a3c364d3dade81cbba30","client_secret":"e1093c6387bfa715f2cb4b6aee010c94deb253ee","admin_user":"cwlseu","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"b53299b4c86ba483491793baed51e4fa"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
